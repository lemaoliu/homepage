## Seminars at LAT Group

| Date   |      Titiles      |  Speakers |
|----------|:-------------:|------:|
| 7-22-2020 |  Pre-training via Paraphrasing <br /> Pretraining with Contrastive Sentence Objectives Improves Discourse Performance of Language Models  | Deng Cai 
| 7-22-2020 |    MASS: Masked Sequence to Sequence Pre-training for Language Generation <br /> BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension <br />  PALM: Pre-training an Autoencoding&Autoregressive Language Model for Context-conditioned Generation  |   Zhangming Chan |


<!---
<table>
    <thead>
        <tr>
            <th>Layer 1</th>
            <th>Layer 2</th>
            <th>Layer 3</th>
        </tr>
    </thead>
    <tbody>
        <tr>
            <td rowspan=4>L1 Name</td>
            <td rowspan=2>L2 Name A</td>
            <td>L3 Name A</td>
        </tr>
        <tr>
            <td>L3 Name B</td>
        </tr>
        <tr>
            <td rowspan=2>L2 Name B</td>
            <td>L3 Name C</td>
        </tr>
        <tr>
            <td>L3 Name D</td>
        </tr>
    </tbody>
</table>
--->
